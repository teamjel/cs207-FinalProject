{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Mwe4dFKJBG2Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Differentiation is one of the most important operations in science.  Finding extrema of functions and determining zeros of functions are central to optimization.  Numerically solving differential equations forms a cornerstone of modern science and engineering and is intimately linked with predictive science.\n",
        "\n",
        "A very frequent occurrence in science requires the scientist to find the zeros of a function ![equation](http://latex.codecogs.com/gif.latex?f%5Cleft%28x%5Cright%29).  The input to the function is a m- dimensional vector and the function returns an n- dimensional vector.  We denote this mathematically as ![equation](http://latex.codecogs.com/gif.latex?f%5Cleft%28x%5Cright%29): ![equation](http://latex.codecogs.com/gif.latex?%5Cmathbb%7BR%7D%5E%7Bm%7D%20%5Cmapsto%20%5Cmathbb%7BR%7D%5E%7Bn%7D).  This expression is read:  the function ![equation](http://latex.codecogs.com/gif.latex?f%5Cleft%28x%5Cright%29) maps ![equation](http://latex.codecogs.com/gif.latex?%5Cmathbb%7BR%7D%5E%7Bm%7D%20%5Cmapsto%20%5Cmathbb%7BR%7D%5E%7Bn%7D).\n",
        "In CS207, we explored the finite difference method, but we also computed a symbolic derivative.  The finite difference approach is nice because it is quick and easy.  However, it suffers from accuracy and stability problems.  On the other hand, symbolic derivatives can be evaluated to machine precision, but can be costly to evaluate.\n",
        "Automatic differentiation (AD) overcomes both of these deficiencies. It is less costly than symbolic differentiation while evaluating derivatives to machine precision.  There are two modes of automatic differentiation: forward and reverse (Lecture 9).  This library will be primarily concerned with the forward mode, but also includes features such as forward mode visualization, reverse mode differentiation, and an application of the differentiation capabilities of our package in Newton fractals. Using this package, one can create a Newton fractal for any polynomial. This is just one way that automatic differentiation is useful in real life!"
      ]
    },
    {
      "metadata": {
        "id": "r4EF-MsFBJvI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# How to Use Package\n",
        "To use, first create a new virtual environment in order to develop with the package without polluting the global environment with dependencies. To do so, install virtualenv with the command `sudo easy_install virtualenv`. Next, go to the top level of your project directory and create a new virtual environment with the command `virtualenv [name]`. To activate the environment, type the command `source [name]/bin/activate`. Thus far, you have set up and activated your dev environment and can begin interacting with the AutoDiff package. To install the package, type in the command line 'pip install autodiff-jel'. You should see an output like so:\n",
        "\n",
        "```\n",
        "Collecting autodiff-jel\n",
        "  Downloading https://files.pythonhosted.org/packages/dd/31/ca8117df436f73a4e686b8410542ea781f4609b3afd09d99f9d9881b58bf/autodiff_jel-0.0.2.tar.gz\n",
        "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from autodiff-jel) (1.14.6)\n",
        "Building wheels for collected packages: autodiff-jel\n",
        "  Running setup.py bdist_wheel for autodiff-jel: started\n",
        "  Running setup.py bdist_wheel for autodiff-jel: finished with status 'done'\n",
        "  Stored in directory: /root/.cache/pip/wheels/21/c5/9b/334502b015b5461601f7c69febf8be158082b3dccbe0f34cbc\n",
        "Successfully built autodiff-jel\n",
        "Installing collected packages: autodiff-jel\n",
        "Successfully installed autodiff-jel-0.0.2\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "y0VQPcuGDwGG",
        "colab_type": "code",
        "outputId": "802c60ed-152d-492b-bd6f-20e0a8026767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install autodiff-jel --upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autodiff-jel\n",
            "  Downloading https://files.pythonhosted.org/packages/30/62/6840afab118ec0b50c049703638f5fb06d79f6a876ee30439154bf2d224b/autodiff_jel-0.0.4.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from autodiff-jel) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.6/dist-packages (from autodiff-jel) (0.10.1)\n",
            "Building wheels for collected packages: autodiff-jel\n",
            "  Running setup.py bdist_wheel for autodiff-jel: started\n",
            "  Running setup.py bdist_wheel for autodiff-jel: finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/79/53/88a171311313de0eae7cf978001a90b5bddd69951f6c5643b8\n",
            "Successfully built autodiff-jel\n",
            "Installing collected packages: autodiff-jel\n",
            "  Found existing installation: autodiff-jel 0.0.3\n",
            "    Uninstalling autodiff-jel-0.0.3:\n",
            "      Successfully uninstalled autodiff-jel-0.0.3\n",
            "Successfully installed autodiff-jel-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TUnH2ofeDwh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Congratulations, you have installed autodiff-jel. To check that the installation was successful, run the python interpreter by typing 'python'. Import the module and do a simple operation such as printing out the name of the package like so:"
      ]
    },
    {
      "metadata": {
        "id": "d6WtO7ULD7JZ",
        "colab_type": "code",
        "outputId": "fa39f4c0-def6-45d8-acd4-eac1ce614ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import autodiff as AD\n",
        "import numpy as np\n",
        "AD.name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'autodiff_jel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "VHeyB7izD5eV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "```Python\n",
        ">>> import autodiff as AD\n",
        ">>> import numpy as np\n",
        ">>> AD.name\n",
        "'autodiff_jel'\n",
        "```\n",
        "\n",
        "If your screen looks like the above, you have successfully installed `autodiff`!\n",
        "\n",
        "Let's now go through a demo. Let us use automatic differentiation on the function sin(x).\n",
        "We first need to set up the variable and the equation."
      ]
    },
    {
      "metadata": {
        "id": "XewxioYGEFmq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = AD.Variable(\"x\")\n",
        "y = AD.sin(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVniLOmZEDPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we need to assign a value to each variable during the evaluation call."
      ]
    },
    {
      "metadata": {
        "id": "jFE_W1O8EQo9",
        "colab_type": "code",
        "outputId": "c7662a84-9fe2-42df-ac31-b2fba1149beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y(x=np.pi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Node(Function = 'Sine', Value = 1.2246467991473532e-16, Derivative = {'x': -1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "ebDiprUDEWvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lastly, we can now get the derivative of the function like so and specify the variable of the partial derivative:\n"
      ]
    },
    {
      "metadata": {
        "id": "jy4gIdxMEZ0x",
        "colab_type": "code",
        "outputId": "82e2763d-ff80-4286-c354-660e3bdf300b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (y.derivative()[\"x\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiGH-V4YLi_I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can create a computational graph and a table for the equation:"
      ]
    },
    {
      "metadata": {
        "id": "yMwIwP1ZLnYh",
        "colab_type": "code",
        "outputId": "59954617-adba-4165-dde7-61e0f67e70e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "y.get_comp_graph()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<autodiff.visualization.CompGraph at 0x7f48649da5c0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"63pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 62.60 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-112 58.5952,-112 58.5952,4 -4,4\"/>\n<!-- 1 -->\n<g id=\"node1\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"#0000ff\" cx=\"27.2976\" cy=\"-90\" rx=\"27.0966\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27.2976\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sine</text>\n</g>\n<!-- 0 -->\n<g id=\"node2\" class=\"node\">\n<title>0</title>\n<polygon fill=\"none\" stroke=\"#ff0000\" points=\"54.2976,-36 .2976,-36 .2976,0 54.2976,0 54.2976,-36\"/>\n<text text-anchor=\"middle\" x=\"27.2976\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M27.2976,-36.1686C27.2976,-43.869 27.2976,-53.0257 27.2976,-61.5834\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"23.7977,-61.5867 27.2976,-71.5867 30.7977,-61.5868 23.7977,-61.5867\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "LuvP22RMVO6_",
        "colab_type": "code",
        "outputId": "4d385f51-ad5e-47d2-a6b2-7871cc54c55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "y.get_comp_table()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Trace</th>\n",
              "      <th>Elementary Function</th>\n",
              "      <th>Current Value</th>\n",
              "      <th>Grad x value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>x_1</td>\n",
              "      <td>x</td>\n",
              "      <td>3.141593e+00</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>x_2</td>\n",
              "      <td>Sine(x_1)</td>\n",
              "      <td>1.224647e-16</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Trace Elementary Function  Current Value  Grad x value\n",
              "0   x_1                   x   3.141593e+00           1.0\n",
              "1   x_2           Sine(x_1)   1.224647e-16          -1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "3ihDNSZNLqBc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Checkout `visualization_examples.ipynb` file under examples folder for more details on visualization."
      ]
    },
    {
      "metadata": {
        "id": "a9PDsqufLg7r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations, you can now begin automatically differentiating away! "
      ]
    },
    {
      "metadata": {
        "id": "b94Z_6LMBXj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "The basic principles behind automatic differentiation rely on calculating the derivative of a function by splitting the calculation into a number of parts, and can be approached using a number of equivalent methodologies. The simplest illustration of the forward mode of AD is taking the chain rule and treating a tricky function as a composite function of a series of elementary operations. The product of each derivative, building up through each of the elementary functions, gives a computationally simple and accurate method for evaluating difficult derivatives.\n",
        "\n",
        "For functions that map ![equation](http://latex.codecogs.com/gif.latex?R%5Em) to ![equation](http://latex.codecogs.com/gif.latex?R%5En), we can see that this method of computing the derivative is equivalent to computing the Jacobian. This can be computationally illustrated through:\n",
        "\n",
        "![equation](http://latex.codecogs.com/gif.latex?D_%7Bp%7Dx%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7Bm%7D%7B%5Cdfrac%7B%5Cpartial%20x%7D%7B%5Cpartial%20x_%7Bj%7D%7Dp_%7Bj%7D%7D)\n",
        "\n",
        "where we take x as our m-dimensional input vector, and ![equation](http://latex.codecogs.com/gif.latex?p) as a seed vector that is all 0's except for a 1 at the dimension we desire to compute for the Jacobian, thus filling in each of the entries.\n",
        "\n",
        "Finally, for the implementation in our software package, we take advantage of the construction of dual numbers, which is an algebra with the following construction:\n",
        "\n",
        "Given any number x, rewrite it as: ![equation](http://latex.codecogs.com/gif.latex?x%20%3D%20x&plus;%5Cepsilon%20x%27), where ![equation](http://latex.codecogs.com/gif.latex?%5Cepsilon) has the property such that ![equation](http://latex.codecogs.com/gif.latex?%5Cepsilon%5E2%20%3D%200). This construction is extremely useful because it enables the automatic computation of derivatives, provided the initial derivative at any given x upon instantiation, simply by expanding the formula and computing algebraically the equivalent dual number solution. For the purposes of our implementation of the forward mode of automatic differentiation, we will use dual numbers to compute the appropriate derivatives at each step. \n",
        "\n",
        "One of the new features in this package is a method that enables the user to visualize the forward propogation of derivatives in forward mode differentiation. The user will be able to visualize graphically how the derivative of the whole function is calculated from the individual components. The visualization comes in the form of a directed acyclic graph, a DAG, and will provide a means to see the propogation of the derivatives from inner function components all the way out. \n",
        "\n",
        "In addition, this library also includes an alternative form of differentiation: reverse mode differentiation. Reverse mode differentiation is a much more efficient method of computing derivatives as compared to forward mode and is a great addition for users concerned with the time complexity of code. This package supports use of reverse mode differentiation for scalar operations.\n",
        "\n",
        "Lastly, another new feature in this package is the ability to visualize Newton fractals. The Newton-Raphson iteration method successively outputs better approximations of the roots of a function through use of the function itself and its derivatives. The method intakes a function *f(x)*, its derivative *f'(x)*, and an approximation of the root a and continually calculates *a′=a−f(a)/f′(a)*, where a' is a better approximation of the root than a. One typically iterates over and over again until the approximations converge, signalling that a really good approximation r has been found where *f(r) = 0*. This method works for roots in both the complex and real planes. Newton fractals build on these approximations and are a visualization of which root each starting root approximation *a* converges to. As one can expect, when beginning approximations are pretty good approximations of a certain root, it will converge to that root. Something interesting happens during when the approximation is a really bad one and it is precisely this that generates the fractals in Newton fractals. These really bad approximations are essentially so bad that the whole surrounding area is essentially unstable and does not converge to one root as a whole. This behavior gives us these beautiful fractals. (https://www.chiark.greenend.org.uk/~sgtatham/newton/ [link text](https://www.chiark.greenend.org.uk/~sgtatham/newton/))\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FX6KvevhBaXy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Software Organization\n",
        "\n",
        "Our directory will look like:\n",
        "```\n",
        "cs207-Finalproject\\\n",
        "  autodiff\\\n",
        "    __init__.py\n",
        "    fractals.py\n",
        "    node.py\n",
        "    operators.py\n",
        "    visualization.py\n",
        "  tests\\\n",
        "    test_node.py\n",
        "    test_operators.py\n",
        "    test_visualization.py\n",
        "  examples\\\n",
        "    visualization_examples.ipynb\n",
        "  docs\\\n",
        "    final.ipynb\n",
        "    milestone1.md\n",
        "    milestone2.md\n",
        "  .gitignore\n",
        "  .travis.yml\n",
        "  LICENSE\n",
        "  setup.py\n",
        "  setup.cfg\n",
        "  README.md\n",
        "  requirements.txt\n",
        "```\n",
        "\n",
        "The key modules and their basic functionalities are:\n",
        "* node.py: Defines the core structure of forward automatic differentiation. This includes operator overloading.\n",
        "* operators.py: Defines operators that can be applied to nodes.\n",
        "* visualization.py: Visualizes the computational graph or table for forward automatic differentation.\n",
        "* fractals.py: Implements and visualizes Newton fractals.\n",
        "\n",
        "The test suite will live in the `tests/` directory, which we will be maintain by using  TravisCI for continuous integration and Coveralls for verifying test coverage. The package will be distributed through PyPI."
      ]
    },
    {
      "metadata": {
        "id": "07PmigBKBbME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How to Use AutoDiff\n",
        "\n",
        "The user will obtain our package through PyPI. The user will first install the package then import it into their project file.\n",
        "Example use:\n",
        "\n",
        "```Python\n",
        "from autodiff import Variable, sin, cos, log, exp\n",
        "\n",
        "a = Variable(\"a\")\n",
        "b = Variable(\"b\")\n",
        "c = Variable(\"c\")\n",
        "d = Variable(\"d\")\n",
        "y = cos((-a)**2/c) - 4*sin(b) * log(exp(d) + 1, 10)\n",
        "\n",
        "y(a = 2, b = 3, c = -1, d = 4)\n",
        "\n",
        "print(round(y.value(),2))\n",
        "-1.64\n",
        "\n",
        "print(round(y.derivative()[\"a\"], 2))\n",
        "3.03\n",
        "\n",
        "print(y)\n",
        "Node(Function = 'Subtraction', Value = -1.638695338498409, Derivative = {'b': 6.910386575432481, 'd': -0.24074123364509895, 'c': 3.027209981231713, 'a': 3.027209981231713})\n",
        "```\n",
        "\n",
        "The user can instantiate multiple nodes and apply any operators outlined in the `Operators` module. The value and the gradients of the node can be accessed by using the `value` and `derivative` methods respectively. Our implementation is meant to be as intuitive as possible, allowing natural manipulation of formula expressions through extensive use of python magic methods, and built in functions for handling the most common math functions. In addition, every node saves its values upon computation at any given point, allowing for more extensive analysis at different points and the capability of implementing a visualization module."
      ]
    },
    {
      "metadata": {
        "id": "hkdkEXIYBf3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Implementation\n",
        "Our implementation centers around the use of the class `Node`, which is an abstract class defining a single operation. Nodes may be `Constant`s and `Variable`s, which are reflexive functions that simply return their value. Every node is equivalently a dual number store, as it contains both the real value part, and the dual differentiated part at that point in the graph. Nodes are built upon one another by the `children` attribute, which contain all the lower-level nodes that are involved in the computation of the current node. This implementation seeks to elegantly reconstruct the basis of automatic differentiation, the computational graph and table, in implementing both the forward and reverse modes, and thus necessitates the storage of the derivative values as they are propagated through the graph."
      ]
    },
    {
      "metadata": {
        "id": "S22sMSc4Bh2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What are the core data structures?\n",
        "The core data structures are `Variable`s, which are symbolic at initialization, and are given a value at computation. Computation is invoked by calling any node directly with either a dictionary or a direct keyword list, where the keys to both are simply the names of the variables at instantiation. The variables needed for any given node are only those which are involved directly in the computation up until that node, meaning the recursive list of all variables involved in that node's children. When computation is called, both the values, and the partial derivatives (for every variable involved), are propagated from the Variables to the node from which computation is called, for the forward mode. The data is stored automatically through use of a decorator factory that makes defining any new operations extremely simple, and pain-free by requiring only numerical computation in subclass implementations of new functions, and no handling of the internals of our implementation."
      ]
    },
    {
      "metadata": {
        "id": "4rAw-znhBj4H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What classes will you implement\n",
        "We implement the base Node class representing a function, which necessitates subclassing and specifically the overriding of the `eval` and `diff` methods. These methods, when combined with the provided `node_decorator`, will automatically pass `(values)` and `(values, diffs)` to `eval` and `diff` respectively, which are lists of the values and immediate derivatives of all children nodes. This means that any user-subclassed custom functions will only need to numerically handle the value computation and dual-number based derivative computation and return that output, and the rest of the implementation will work. Furthermore, we use a seed-based derivative system where partials are computed one at a time (essentially passing all requisite variables a one-hot kind of vector in their derivatives to compute one partial), meaning that implementations of the derivative can remain univariate in output, simplifying computation."
      ]
    },
    {
      "metadata": {
        "id": "ML_czLnlBlyr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What method and name attributes will your classes have?\n",
        "\n",
        "`Node`\n",
        "\n",
        "The Node class is the core structure. It is the basis of all classes in `Operators` module. Below are the method and name attributes of the `Node` class.\n",
        "\n",
        "```\n",
        "class Node:\n",
        "  Attributes:\n",
        "    _value: Value at the current node; holds values for the most recent computation\n",
        "    _derivative: Derivative/Gradients of the node in dictionary form\n",
        "    _variables: All variables involved in the computation of this node\n",
        "    _cur_var: Marker for determining the current partial being computed when iterating through all seed values (in computing full Jacobian)\n",
        "    children: A list of all children nodes which are involved in this computation\n",
        "    type: String describing the type of computation or node this is\n",
        "  Methods:\n",
        "    ### Class methods ###\n",
        "    @classmethod\n",
        "    make_constant(cls, value): Class method for constructing a Constant node\n",
        "\n",
        "    @classmethod\n",
        "    make_node(cls, node, *values): Important class method that takes in a new Node instance, and properly instantiates it with children from the unpacked values argument list (which can include both numeric values and nodes)\n",
        "\n",
        "    ### Magic Methods ###\n",
        "    __call__(self): Convenience wrapper for calling the compute function, which computes the node value and derivatives at given point\n",
        "    __repr__(self): Representation of node with values, derivatives, and type of function\n",
        "    __add__(self, value): Constructs an Addition node\n",
        "    __radd__(self, value): ^\n",
        "    __neg__(self): Constructs a Negation node\n",
        "    __sub__(self, value): Constructs a Subtraction node\n",
        "    __rsub__(self, value): ^\n",
        "    __mul__(self, value): Constructs a Multiplication node\n",
        "    __rmul__(self, value): ^\n",
        "    __truediv__(self, value): Constructs a Division node\n",
        "    __rtruediv__(self, value): ^\n",
        "    __pow__(self, value): Constructs a Power node\n",
        "    __rpow__(self, value): ^\n",
        "    __eq__(self, value): Returns true if nodes have the same values and derivatives\n",
        "    __ne__(self, value): Returns not __eq__(self, value)\n",
        "    __hash__(self): Returns unique identification number for each Node for hashing\n",
        "\n",
        "    ### Attribute Methods ###\n",
        "    value(self): Function for returning the value at the current node\n",
        "    derivative(self): Function for returning the derivatives at the current node\n",
        "    set_value(self, value): Set a value\n",
        "    set_derivative(self, value): Set a derivative\n",
        "    set_children(self, *children): Give current node children\n",
        "\n",
        "    ### Variable Methods ###\n",
        "    update_variables(self): Called when constructing a new node. This computes the minimal set of variables involved among the children, and sets the current node's variables reference appropriately\n",
        "    set_variables(self, input_dict): Called at computation, and sets all variables to the given values defined by input_dict. Note that the same minimal set of variables is referenced by all nodes that use it, so this function can be called from anywhere further in the computational graph\n",
        "    zero_vector_derivative: [TODO]\n",
        "    update_cur_var(self): Find the current partial by looking at which variable has been seeded properly. This is necessary as a way to let other nodes not directly calling the compute method what variable the current partial is with regard to.\n",
        "    iterate_seeds(self): A generator that is responsible for iterating among all partials necessary at the current node to find the full gradient\n",
        "\n",
        "    ### Computation Methods ###\n",
        "    compute(self, *args, **kwargs): Method that initates the full computation through all children by taking in either an input dictionary (such as {'x': 4, 'y': 3}), or keyword pairs (such as (x=4, y=3)) with the variables referenced by the name they were instantiated with. Returns self once all values are updated.\n",
        "    eval(self, values): A method to be overriden by subclasses, which will implement the actual calculation of the value itself by the function this node is responsible for. Usage of the decorator node_decorate greatly simplifies this implementation, see specifics below\n",
        "    diff(self, values, diffs): Like eval, to be overriden and implemented with the dual-number solution to the derivative of the current function.\n",
        "    get_comp_graph(self): Creates a computational graph for a given node.\n",
        "    get_comp_table(self): Creates a computational table for a given node.\n",
        "\n",
        "class node_decorate:\n",
        "  This is a decorator implemented as a class (can be implemented as a function, but less elegant) that serves as a decorator factory for methods that need to be overrided: eval, and diff. This class allows all subclasses of Node to only worry about implementing a purely numerical method for computation of values and derivatives, and will handle all the logistics necessary to both save those values at each point in the graph automatically, and properly expose and propagate data as necessary to the function.\n",
        "\n",
        "class Variable:\n",
        "  Subclass of Node that implements a simple Variable. Basis of all computation in forward-mode.\n",
        "class Constant:\n",
        "  Convenience class constructed automatically when a constant shows up in computation.\n",
        "class Addition, Subtraction ...\n",
        "  Subclasses of Node that implement elementary functions.\n",
        "```\n",
        "\n",
        "`Operators`\n",
        "\n",
        "Classes here contain additional Node types that define common operations such as sin, log, exp, etc. These also contain the constructor for these nodes - recall that Node is simply the symbolic representation of a function in the graph, and not an actual computation directly initializable by the user. For that, a more familiar and intuitive approach is provided by the built in functions (denoted with lowercase letters) sin, log, exp, etc. that will take in either numeric values or nodes, and output the appropriate respective Sin, Log, Exp nodes."
      ]
    },
    {
      "metadata": {
        "id": "2cLCgTEABrwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What external dependencies will you rely on?\n",
        "\n",
        "We rely mostly on numpy for efficient computation; other requirements are needed for some of the extension features of this library. For forward mode visualization, we require graphviz and pandas.\n",
        "\n",
        "We will specifically leverage [matrix operations](https://docs.scipy.org/doc/numpy-1.15.1/reference/arrays.html) and [universal functions](https://docs.scipy.org/doc/numpy-1.15.1/reference/ufuncs.html) from `Numpy`, [Digraph](https://networkx.github.io/documentation/networkx-1.10/reference/classes.digraph.html) from graphviz and [Dataframe](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html) from pandas. "
      ]
    },
    {
      "metadata": {
        "id": "RNtjCgJeBsaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How will you deal with elementary functions like sin and exp?\n",
        "\n",
        "Elementary functions including trigonometric functions, logarithmic functions, and exponential functions, will be accounted for in `Operators.py`, which are subclasses of `Node`. These will be naturally handled and can be user overriden by direct import, so that they can use intuitive expressions like `sin(x+y/4)`"
      ]
    },
    {
      "metadata": {
        "id": "Ms2bvE1OB8T5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Future\n",
        "\n",
        "In the future, we can add vector support for reverse mode differentiation. This addition would make our differentiation model truly complete with support for scalar and vector operations in both modes of differentiation. In addition, we could also provide further support for trigonometric functions by including secant, cosecant, and cotangent functions. In addition to the purely mathematical additions to the library, we could also add more extensions for applications of automatic differentiation. One extension could be to add support to minimize or maximize equations given a set of constraints. Such optimization problems come in many forms and in many fields, from artificial intelligence to [topology optimization](https://caeai.com/blog/what-topology-optimization-and-why-use-it), which aims to find the best distribution of a material given a set of contraints and optimization goal. Providing support for optimization problems will open this library up to many different use cases."
      ]
    },
    {
      "metadata": {
        "id": "MBfAeKOCBHWp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}